{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/work/adapting-CLIP-VGPs\n"
     ]
    }
   ],
   "source": [
    "%cd /work/adapting-CLIP-VGPs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6666666666666666"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def Jaccard_sim(x, y):\n",
    "    phrase_set1 = set(x.split('+'))\n",
    "    phrase_set2 = set(y.split('+'))\n",
    "\n",
    "    intersect = len(phrase_set1.intersection(phrase_set2))\n",
    "    union = len(phrase_set1.union(phrase_set2))\n",
    "    return intersect/union\n",
    "\n",
    "Jaccard_sim('blue+hard+hat','hard+hat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80813it [00:00, 85492.60it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import csv\n",
    "\n",
    "val_path = '/work/adapting-CLIP-VGPs/data/flickr/phrases_data/phrase_pair_remove_trivial_match_val.csv'\n",
    "\n",
    "phrases = []\n",
    "scores = []\n",
    "gt = []\n",
    "\n",
    "with open(val_path, encoding='utf-8') as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for i, row in tqdm(enumerate(reader)):\n",
    "        phrase1 = row['phrase1']\n",
    "        phrase2 = row['phrase2']\n",
    "        label = row['ytrue']\n",
    "        scores.append(Jaccard_sim(phrase1, phrase2))\n",
    "        phrases.append([phrase1, phrase2])\n",
    "        gt.append(row['ytrue']=='True')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "thres = 0\n",
    "np_scores = np.array(scores)\n",
    "preds = np_scores>thres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.6170108429139649\n",
      "Precision: 0.7469656992084432\n",
      "Recall: 0.5255731922398589\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "f1 = f1_score(gt, preds)\n",
    "prec = precision_score(gt, preds)\n",
    "rec = recall_score(gt, preds)\n",
    "\n",
    "print(f'F1 Score: {f1}')\n",
    "print(f'Precision: {prec}')\n",
    "print(f'Recall: {rec}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_f1(scores, gt):\n",
    "    step = 0.001\n",
    "    cur_best_f1 = 0\n",
    "    cur_prec = 0\n",
    "    cur_rec = 0\n",
    "    cur_best_thres = 0\n",
    "    for thres in np.arange(cur_best_thres, 1, step):\n",
    "        new_pred = scores > thres\n",
    "        f1 = f1_score(gt, new_pred)\n",
    "\n",
    "        if f1 > cur_best_f1:\n",
    "            cur_best_thres = thres\n",
    "            cur_best_f1 = f1\n",
    "            cur_prec = precision_score(gt, new_pred)\n",
    "            cur_rec = recall_score(gt, new_pred)\n",
    "            \n",
    "    return cur_best_thres, cur_best_f1, cur_prec, cur_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 0.6170108429139649, 0.7469656992084432, 0.5255731922398589)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_best_f1(np_scores, gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "37155it [00:00, 93646.00it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "81285it [00:00, 93533.83it/s]\n"
     ]
    }
   ],
   "source": [
    "thres = 0\n",
    "\n",
    "test_path = '/work/adapting-CLIP-VGPs/data/flickr/phrases_data/phrase_pair_remove_trivial_match_test.csv'\n",
    "\n",
    "# phrases = []\n",
    "# scores = []\n",
    "preds = []\n",
    "gt = []\n",
    "\n",
    "with open(test_path, encoding='utf-8') as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for i, row in tqdm(enumerate(reader)):\n",
    "        phrase1 = row['phrase1']\n",
    "        phrase2 = row['phrase2']\n",
    "        # label = row['ytrue']\n",
    "        score = Jaccard_sim(phrase1, phrase2)\n",
    "        preds.append(score>thres)\n",
    "        # phrases.append([phrase1, phrase2])\n",
    "        gt.append(row['ytrue']=='True')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.6124667137655562\n",
      "Precision: 0.7413498223917906\n",
      "Recall: 0.5217592592592593\n"
     ]
    }
   ],
   "source": [
    "f1 = f1_score(gt, preds)\n",
    "prec = precision_score(gt, preds)\n",
    "rec = recall_score(gt, preds)\n",
    "\n",
    "print(f'F1 Score: {f1}')\n",
    "print(f'Precision: {prec}')\n",
    "print(f'Recall: {rec}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
