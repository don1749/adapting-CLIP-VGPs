{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/work/adapting-CLIP-VGPs\n"
     ]
    }
   ],
   "source": [
    "%cd /work/adapting-CLIP-VGPs/\n",
    "import clip\n",
    "\n",
    "GPU = 7\n",
    "model, _ = clip.load('ViT-L/14', device=GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLIP(\n",
       "  (visual): VisionTransformer(\n",
       "    (conv1): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
       "    (ln_pre): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    (transformer): Transformer(\n",
       "      (resblocks): Sequential(\n",
       "        (0): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (6): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (7): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (8): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (9): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (10): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (11): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (12): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (13): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (14): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (15): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (16): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (17): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (18): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (19): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (20): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (21): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (22): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (23): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_post): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (transformer): Transformer(\n",
       "    (resblocks): Sequential(\n",
       "      (0): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (1): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (2): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (3): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (4): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (5): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (6): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (7): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (8): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (9): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (10): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (11): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (token_embedding): Embedding(49408, 768)\n",
       "  (ln_final): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.vgp_data import FlickrVGPsDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "test_dataset = FlickrVGPsDataset(data_type='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Several climbers', 'Seven climbers']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo_phrases = test_dataset[0]['phrases']\n",
    "demo_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[49406,  5560, 47648, 49407,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0],\n",
       "        [49406,  5757, 47648, 49407,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0]], device='cuda:7',\n",
       "       dtype=torch.int32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrases_tensor = clip.tokenize(demo_phrases).to(GPU)\n",
    "phrases_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2708,  0.2357, -0.0625,  ...,  0.1311, -0.4551, -0.2832],\n",
       "        [-0.5215,  0.1737, -0.4531,  ...,  0.1414, -0.0684, -0.7246]],\n",
       "       device='cuda:7', dtype=torch.float16, grad_fn=<MmBackward>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_ft = model.encode_text(phrases_tensor)\n",
    "text_ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8868458468103846"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "cosine_similarity(text_ft.cpu().detach())[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_cos_sim(phrases):\n",
    "    phrases_tensor = clip.tokenize(phrases).to(GPU)\n",
    "    text_ft = model.encode_text(phrases_tensor)\n",
    "    return cosine_similarity(text_ft.cpu().detach())[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm import tqdm\n",
    "\n",
    "# thres = 0.7\n",
    "# phrase_pairs = []\n",
    "# scores = []\n",
    "# preds = []\n",
    "# gt = []\n",
    "# for data in tqdm(test_dataset):\n",
    "#     phrases = data['phrases']\n",
    "#     phrase_pairs.append(phrases)\n",
    "\n",
    "#     score = text_cos_sim(phrases)\n",
    "#     scores.append(score)\n",
    "\n",
    "#     preds.append(score>thres)\n",
    "#     gt.append(data['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# # Replace actual_labels and predicted_labels with your own data\n",
    "# cm = confusion_matrix(gt, preds)\n",
    "\n",
    "# # Define class labels\n",
    "# class_labels = ['Negative', 'Positive']\n",
    "\n",
    "# # Create a heatmap of the confusion matrix\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# sns.set(font_scale=1.2)  # Adjust font size\n",
    "# sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_labels, yticklabels=class_labels, cbar=False)\n",
    "\n",
    "# plt.xlabel('Predicted')\n",
    "# plt.ylabel('Actual')\n",
    "# plt.title('Confusion Matrix')\n",
    "# plt.show()\n",
    "\n",
    "# from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# # Calculate precision\n",
    "# precision = precision_score(gt, preds)\n",
    "\n",
    "# # Calculate recall\n",
    "# recall = recall_score(gt, preds)\n",
    "\n",
    "# # Calculate F1 score\n",
    "# f1 = f1_score(gt, preds)\n",
    "\n",
    "# # Calculate accuracy\n",
    "# acc = sum(x==y for x,y in zip(gt,preds))/len(gt)\n",
    "\n",
    "# print(f'Accuracy: {acc}')\n",
    "# print(f'Precision: {precision}')\n",
    "# print(f'Recall: {recall}')\n",
    "# print(f'F1 Score: {f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([768])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_ft[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Several climbers', 'a rock face']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo_phrases = test_dataset[1]['phrases']\n",
    "demo_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-5.3223e-01, -6.5771e-01, -8.0078e-02, -1.8982e-01, -2.0654e-01,\n",
       "          1.2177e-01,  3.6108e-01,  2.6489e-01, -2.2693e-01, -5.4092e-03,\n",
       "          1.0187e-01, -6.9922e-01, -5.8984e-01,  4.3384e-01, -5.2100e-01,\n",
       "         -2.6538e-01,  9.9170e-01,  3.7598e-01,  3.3228e-01, -3.9258e-01,\n",
       "         -1.8970e-01,  2.4890e-01,  1.5710e-01, -1.2427e-01, -3.1396e-01,\n",
       "         -4.3823e-01,  1.0146e+00, -2.2913e-01,  5.5756e-02,  1.8262e-01,\n",
       "         -6.7139e-01, -2.5342e-01,  4.1455e-01,  5.0977e-01,  1.7346e-01,\n",
       "         -2.0117e-01, -2.8870e-02, -1.2354e-01, -7.3975e-02, -2.6807e-01,\n",
       "         -6.3591e-03,  1.5613e-01,  4.0625e-01, -4.5532e-01, -1.2695e-01,\n",
       "         -6.0364e-02,  3.7939e-01, -2.0752e-01,  5.1855e-01, -3.8208e-01,\n",
       "          2.5513e-01,  3.2379e-02,  2.3682e-01, -3.3984e-01,  2.4683e-01,\n",
       "         -1.5625e-01,  1.7664e-01,  3.8013e-01, -7.4463e-02, -1.2427e-01,\n",
       "          5.6445e-01,  7.2803e-01, -6.9092e-01,  1.3782e-01, -1.1709e+00,\n",
       "          7.4463e-01,  3.2764e-01,  1.6211e-01, -1.3525e-01, -3.3276e-01,\n",
       "         -4.5020e-01,  2.1347e-02, -2.9395e-01,  5.4932e-01, -1.5601e-01,\n",
       "          3.4326e-01,  1.6943e-01,  4.9530e-02,  5.9509e-02, -7.1680e-01,\n",
       "          1.5900e-02, -2.2949e-01, -1.2573e-01,  5.7520e-01, -5.6213e-02,\n",
       "         -1.5710e-01, -6.9153e-02, -3.0933e-01, -9.2773e-01,  3.8940e-02,\n",
       "          2.0728e-01,  3.2764e-01, -2.0947e-01, -4.3359e-01, -1.6983e-02,\n",
       "         -5.3369e-01, -9.2224e-02, -2.9224e-01, -1.2781e-01, -3.5425e-01,\n",
       "          3.4729e-02,  3.8940e-02,  2.3926e-01,  7.2070e-01,  3.1519e-01,\n",
       "          1.2321e-02,  4.5215e-01, -1.8433e-01, -6.3086e-01, -2.5928e-01,\n",
       "         -2.5830e-01,  4.5898e-01,  1.1176e-01,  1.7773e-01, -1.0370e-01,\n",
       "         -2.6343e-01, -6.3281e-01,  2.5806e-01,  9.8486e-01, -5.1855e-01,\n",
       "         -2.0776e-01, -2.2058e-01,  4.4629e-01, -3.1372e-01,  2.6489e-01,\n",
       "          4.7241e-01, -5.0293e-02, -5.1221e-01,  9.9792e-02, -2.7051e-01,\n",
       "         -4.7485e-02, -2.7173e-01,  2.5708e-01, -1.9348e-01, -3.5693e-01,\n",
       "          3.4448e-01, -2.6215e-02, -5.6427e-02,  3.5449e-01, -2.4768e-01,\n",
       "          2.8320e-01,  4.5197e-02, -1.1151e-01,  7.0166e-01,  2.2354e-02,\n",
       "          5.7411e-03,  3.2568e-01,  1.4824e-02, -3.2080e-01,  2.6123e-01,\n",
       "         -4.5837e-02, -2.7466e-01,  5.0293e-01, -6.3135e-01,  1.1926e-01,\n",
       "          3.3295e-02, -1.6528e-01, -3.9575e-01, -2.9688e-01, -3.8232e-01,\n",
       "         -5.3955e-01, -2.0691e-01,  1.7042e-03,  3.7915e-01,  1.1871e-02,\n",
       "          5.3467e-02, -9.8206e-02, -3.1079e-01,  1.5295e-01, -3.9185e-01,\n",
       "          5.0049e-01,  2.3718e-01, -3.0518e-01,  1.3879e-01,  6.1768e-01,\n",
       "         -2.1167e-01, -1.7969e-01, -9.4580e-01,  6.0938e-01,  1.9849e-01,\n",
       "         -2.2449e-01,  2.4231e-02,  9.7961e-02,  1.0394e-01,  3.2898e-02,\n",
       "         -4.3188e-01, -3.3350e-01, -7.2693e-02,  5.4932e-04, -2.2717e-01,\n",
       "         -1.2482e-01, -4.4043e-01,  8.1738e-01, -3.1348e-01,  1.2256e+00,\n",
       "         -1.6777e+00, -3.3911e-01,  3.5962e-01,  6.4062e-01, -6.9727e-01,\n",
       "         -1.1060e-01,  7.4756e-01, -2.9272e-01, -5.4102e-01, -1.2976e-01,\n",
       "          1.8823e-01, -5.9387e-02, -3.6890e-01,  2.0239e-01,  3.1543e-01,\n",
       "         -7.3389e-01, -2.2754e-01,  5.6201e-01, -2.5537e-01, -2.0312e-01,\n",
       "          1.4709e-01, -3.6841e-01,  2.0905e-02,  3.2471e-01,  8.6328e-01,\n",
       "         -7.2266e-01, -4.8315e-01,  6.9885e-02, -5.6152e-01, -5.6885e-01,\n",
       "          5.4901e-02,  3.2330e-03, -8.8330e-01,  6.4062e-01, -9.5605e-01,\n",
       "          7.6111e-02,  4.0833e-02, -2.0569e-01, -1.3794e-01, -3.9642e-02,\n",
       "          1.9983e-01,  1.9287e-01,  5.2100e-01,  7.1777e-01,  1.9434e-01,\n",
       "          4.7339e-01, -5.8398e-01,  1.0425e-01, -3.4375e-01, -2.4084e-01,\n",
       "          7.1680e-01, -2.1790e-02,  1.6370e-01, -1.0242e-01,  4.8315e-01,\n",
       "          1.1847e-01, -3.5767e-01,  8.2886e-02, -5.2783e-01,  1.3342e-01,\n",
       "          1.7358e-01,  2.7002e-01,  6.7773e-01, -2.9932e-01, -2.2571e-01,\n",
       "         -1.2659e-01,  1.2510e+00,  2.1155e-01,  1.9739e-01, -3.1958e-01,\n",
       "          3.0908e-01,  1.2250e-01, -9.9243e-02, -3.5889e-01,  1.2671e-01,\n",
       "          7.5146e-01, -1.2805e-01,  1.4722e-01, -2.7930e-01,  2.1289e-01,\n",
       "         -2.5464e-01, -4.5874e-01,  2.2949e-01, -2.6782e-01, -8.0627e-02,\n",
       "          3.9380e-01,  7.3926e-01, -4.4287e-01,  2.3413e-01,  2.0767e-02,\n",
       "          1.3269e-01,  8.1909e-02,  2.7218e-03, -8.5205e-01, -2.9712e-01,\n",
       "         -2.5415e-01, -4.3121e-02,  4.3262e-01,  4.6722e-02, -5.1855e-01,\n",
       "          2.1741e-01, -6.2549e-01, -6.8787e-02, -1.0980e-01,  1.6235e-01,\n",
       "         -4.3365e-02, -3.4912e-01,  2.0154e-01, -1.6309e-01,  5.3613e-01,\n",
       "          4.8438e-01,  1.6602e-01,  1.7773e-01,  5.6190e-03,  6.9434e-01,\n",
       "          7.7637e-02, -1.1194e-01, -2.0605e-01,  2.8882e-01,  4.6826e-01,\n",
       "         -4.6729e-01, -8.8281e+00,  3.8361e-02, -6.3281e-01,  5.7471e-01,\n",
       "         -2.5513e-01, -8.9014e-01, -1.5991e-01,  3.8086e-01,  3.3130e-01,\n",
       "          2.9999e-02, -8.8135e-01,  2.2522e-01, -1.0669e-01,  2.5220e-01,\n",
       "         -5.0635e-01,  6.7139e-01,  8.9905e-02,  3.3911e-01, -6.2439e-02,\n",
       "         -2.7197e-01, -5.1318e-01, -3.3936e-01, -3.5791e-01, -2.7390e-02,\n",
       "         -2.5146e-01, -2.0068e-01,  3.0859e-01,  4.0894e-01, -2.6807e-01,\n",
       "          6.2354e-01,  3.8989e-01,  6.8176e-02, -3.6426e-01, -3.4692e-01,\n",
       "          1.5515e-01, -2.9590e-01, -2.2656e-01,  9.6558e-02,  6.4453e-02,\n",
       "         -5.2155e-02, -3.3594e-01, -6.8408e-01,  8.3252e-01, -5.0977e-01,\n",
       "          5.2643e-02, -6.6895e-01, -1.2134e-01, -4.6484e-01,  4.9072e-01,\n",
       "          6.4636e-02,  3.7158e-01, -2.1021e-01, -2.5726e-02,  1.6663e-01,\n",
       "         -2.7657e-03, -7.2998e-02, -1.1920e-01,  1.7529e-01, -1.4429e-01,\n",
       "         -9.5703e-02, -5.3619e-02,  3.5425e-01,  5.6738e-01, -1.7712e-01,\n",
       "         -1.5186e-01, -5.3369e-01, -4.7827e-01,  3.7012e-01,  3.1567e-01,\n",
       "          6.8970e-02,  2.1606e-02, -1.7480e-01,  6.3428e-01,  7.9529e-02,\n",
       "         -3.5400e-01,  7.0648e-03, -3.4253e-01, -1.7615e-01, -7.4219e-01,\n",
       "          4.4458e-01,  1.7981e-01,  3.4668e-01, -8.3542e-03, -1.2830e-01,\n",
       "          8.2861e-01,  8.6914e-02, -1.7651e-01,  4.9042e-02,  4.8096e-01,\n",
       "          1.5125e-01, -9.5654e-01,  5.0140e-02, -1.7566e-01,  6.5723e-01,\n",
       "         -3.0566e-01, -4.6265e-01, -3.2013e-02, -9.5264e-01, -8.8318e-02,\n",
       "         -2.8784e-01,  2.2986e-01,  9.9121e-02, -7.0740e-02,  2.6978e-01,\n",
       "         -4.6899e-01, -4.1406e-01, -9.4604e-02,  1.4685e-01, -2.1448e-01,\n",
       "         -4.9927e-01, -8.9697e-01, -8.0750e-02,  1.5808e-02,  8.9404e-01,\n",
       "         -4.7211e-02, -4.6167e-01, -1.0394e-01,  9.1248e-02, -3.0054e-01,\n",
       "          5.8057e-01, -2.2058e-01, -7.6758e-01, -1.8079e-01, -2.7515e-01,\n",
       "          8.2344e+00, -1.6162e-01, -4.8193e-01,  4.2065e-01,  4.3579e-01,\n",
       "         -3.1958e-01,  6.3818e-01,  1.5612e-03,  1.6455e-01,  5.7568e-01,\n",
       "         -6.9702e-02, -2.6207e-03,  5.2051e-01,  6.2012e-01, -4.1846e-01,\n",
       "         -2.3608e-01, -8.6426e-02,  5.4004e-01, -2.4292e-02, -3.8574e-02,\n",
       "          2.7588e-01, -6.4087e-02, -5.4443e-01,  8.7500e-01,  4.0375e-02,\n",
       "         -4.3213e-02,  3.8477e-01, -1.8408e-01, -8.4595e-02, -1.1151e-01,\n",
       "         -4.6313e-01, -3.4058e-01,  1.0358e-01, -1.6077e-01, -4.5898e-01,\n",
       "          2.2485e-01,  4.4525e-02, -4.4165e-01,  5.2734e-01,  3.1104e-01,\n",
       "          7.1289e-01, -7.6782e-02, -6.7676e-01,  2.2546e-01, -2.4915e-01,\n",
       "         -4.1260e-01,  3.8184e-01,  2.0837e-01,  4.7192e-01,  5.2148e-01,\n",
       "         -1.2354e+00, -5.3558e-02, -4.7559e-01,  1.9189e-01, -6.4148e-02,\n",
       "          3.2495e-01, -1.9910e-01, -8.4229e-01, -3.5034e-01, -8.9600e-02,\n",
       "          1.4392e-01, -2.2961e-01,  1.0371e+00,  8.0322e-01, -2.7954e-01,\n",
       "         -6.1133e-01, -1.8640e-01,  1.8188e-01, -4.8267e-01,  3.3862e-01,\n",
       "         -2.0935e-02,  6.6797e-01, -3.1543e-01, -1.6357e-01, -4.7339e-01,\n",
       "          1.3147e-01,  6.3477e-01,  4.3091e-01,  2.5293e-01,  4.6191e-01,\n",
       "          4.3311e-01,  2.6514e-01,  1.9324e-01,  4.0845e-01,  3.0664e-01,\n",
       "          2.2003e-02, -6.3843e-02, -5.5176e-02, -4.5874e-01, -2.8003e-01,\n",
       "         -3.9404e-01, -3.3813e-02, -3.7628e-02, -8.8928e-02,  1.8542e-01,\n",
       "         -4.3652e-01, -4.4629e-01, -7.2327e-02,  3.2202e-01, -2.9614e-01,\n",
       "          2.4512e-01,  6.9122e-03, -1.1060e-01,  3.2666e-01,  4.3701e-01,\n",
       "         -2.0264e-01,  3.1079e-01,  5.0098e-01, -3.0688e-01,  4.3335e-01,\n",
       "         -1.7322e-01,  1.5845e-01,  3.8525e-01,  2.4368e-02,  1.5190e-02,\n",
       "          4.1211e-01, -5.7471e-01, -1.5100e-01, -9.0942e-02,  1.2469e-01,\n",
       "         -7.2205e-02, -1.9409e-01,  1.9580e-01,  2.1045e-01,  5.8545e-01,\n",
       "          2.2913e-01,  3.0396e-01, -1.4038e-01,  4.3286e-01, -2.3840e-01,\n",
       "          7.1240e-01, -1.3313e-02, -1.4160e-01,  4.6875e-01, -5.6006e-01,\n",
       "          3.2715e-01,  3.3875e-02, -3.7451e-01,  5.1709e-01, -1.1511e-01,\n",
       "         -5.9753e-02,  6.6699e-01, -3.1445e-01, -6.1584e-02, -4.1235e-01,\n",
       "         -3.1934e-01,  7.8064e-02, -7.8467e-01,  4.9194e-02, -3.8116e-02,\n",
       "         -4.9048e-01,  2.3718e-01,  5.2368e-02,  2.3193e-01,  1.2280e-01,\n",
       "          7.2937e-02, -4.1943e-01, -2.8638e-01, -3.0249e-01,  1.4404e-01,\n",
       "         -6.7871e-01, -4.7241e-01, -1.7529e-01,  3.7573e-01, -2.4316e-01,\n",
       "         -6.9727e-01, -4.4507e-01,  1.3159e-01, -9.1980e-02,  1.8994e-01,\n",
       "          1.5295e-01,  4.0430e-01, -6.3965e-01, -1.2744e-01,  4.4507e-01,\n",
       "          4.0796e-01, -4.1772e-01, -1.9409e-01,  6.8018e-01,  2.2986e-01,\n",
       "         -8.9111e-02,  5.0195e-01, -2.6685e-01,  1.5857e-01, -6.5234e-01,\n",
       "          5.2197e-01,  2.3157e-01, -8.4180e-01,  2.3956e-02, -1.3770e-01,\n",
       "          4.0918e-01,  4.3701e-02, -3.3911e-01,  1.8274e-01, -5.1074e-01,\n",
       "         -3.7689e-02,  1.6443e-01, -1.1450e-01, -3.3618e-01,  1.6675e-01,\n",
       "         -8.1665e-02,  1.9299e-01,  8.8037e-01, -3.3008e-01,  9.6777e-01,\n",
       "          3.5938e-01,  3.8086e-01,  9.4788e-02,  2.0782e-02,  3.2495e-01,\n",
       "         -6.2988e-01,  4.2458e-03,  2.4109e-01,  8.2153e-02, -1.3660e-01,\n",
       "          4.3530e-01,  9.2102e-02, -2.3865e-02,  3.3105e-01,  4.2944e-01,\n",
       "         -2.0520e-01, -5.8643e-01,  4.5752e-01, -1.9897e-01,  8.1665e-02,\n",
       "          6.3477e-02, -5.8984e-01,  7.1631e-01,  5.8008e-01,  7.0752e-01,\n",
       "          2.4854e-01,  2.4255e-01,  3.1885e-01, -1.1322e-01,  2.3242e-01,\n",
       "         -3.6157e-01,  7.6709e-01,  1.5161e-01, -8.2080e-01, -6.5820e-01,\n",
       "          3.4302e-01, -3.4790e-02,  5.5176e-02, -4.5117e-01, -1.1896e-01,\n",
       "         -2.9712e-01,  3.7231e-01, -5.7129e-01,  3.4741e-01, -1.8335e-01,\n",
       "         -3.2715e-02,  9.0088e-02, -1.9177e-01,  1.9092e-01, -1.8250e-01,\n",
       "          3.4546e-01,  2.1704e-01,  3.7329e-01,  4.6313e-01, -3.8550e-01,\n",
       "         -3.9819e-01,  5.0732e-01,  2.5610e-01, -1.1592e+00,  1.2372e-01,\n",
       "          2.5244e-01, -2.9617e-02, -4.6753e-01, -1.8628e-01,  8.3008e-01,\n",
       "          5.0195e-01, -7.5867e-02, -2.8833e-01, -3.5474e-01, -2.2998e-01,\n",
       "         -3.1433e-02,  1.9482e-01, -3.1714e-01,  9.5032e-02, -2.1399e-01,\n",
       "          5.1074e-01, -1.9806e-02, -6.0303e-02, -2.2424e-01,  1.2676e+00,\n",
       "          3.7891e-01,  6.8457e-01,  3.7402e-01, -1.4484e-04,  1.8909e-01,\n",
       "          1.9409e-01,  1.9983e-01,  2.8870e-02,  2.5787e-02,  3.2446e-01,\n",
       "          2.1500e-02,  4.7144e-01, -4.9316e-01, -1.4417e-01,  1.2891e-01,\n",
       "          3.6768e-01,  3.3350e-01, -3.3105e-01,  1.1334e-01, -9.8389e-02,\n",
       "          3.5303e-01,  8.9258e-01, -2.7002e-01, -4.3848e-01,  3.6523e-01,\n",
       "         -1.8262e-01, -3.0029e-01, -1.8286e-01,  2.3315e-01,  1.3664e-02,\n",
       "         -8.7451e-01,  3.6743e-01,  3.7720e-01,  2.2253e-01, -1.7822e-01,\n",
       "          3.6475e-01,  7.0435e-02,  3.5126e-02,  5.1807e-01,  2.3279e-01,\n",
       "         -3.1372e-01, -3.1665e-01, -2.3486e-01]], device='cuda:7',\n",
       "       dtype=torch.float16, grad_fn=<MmBackward>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo_phrase = demo_phrases[1]\n",
    "phr_tkn = clip.tokenize(demo_phrase).to(GPU)\n",
    "phr_ft = model.encode_text(phr_tkn)\n",
    "phr_ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([768])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phr_ft[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "emb = np.load('/work/adapting-CLIP-VGPs/data/flickr/phrase_embs/wet hair.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 768)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29M\t/work/adapting-CLIP-VGPs/data/flickr/phrase_embs\n"
     ]
    }
   ],
   "source": [
    "! du -h /work/adapting-CLIP-VGPs/data/flickr/phrase_embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "emb_tensor = torch.from_numpy(emb)\n",
    "emb_tensor.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
